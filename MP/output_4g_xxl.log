Overriding config with /pscratch/sd/e/es_lh/nanoGPT/config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
block_size = 256 # context of up to 256 previous characters
gradient_accumulation_steps = 4 # 1 gpu: 1 # 1 node: 4 # 2 nodes: 8
batch_size = 16 # 1 gpu: 64 # 1 node: 16 # 2 nodes: 8

# baby GPT model :)
n_layer = 48
n_head = 32 # to make it balance with tp
n_embd = 2048
dropout = 0.2

learning_rate = 1e-4 # with baby networks can afford to go a bit higher # 1e-3 initially
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-5 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

model_parallel = 4
tensor_parallel = 4

Overriding: compile = False
tokens per iteration will be: 16,384
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 2416.25M
[ModelParallelGPT] stages per device: [12, 12, 12, 12]
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:0: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:1: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:2: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
  - Block@device cuda:3: LayerNorm
[Stage 0] 0.ln_1.weight -> cuda:0
[Stage 0] 0.attn.c_attn.weight -> cuda:0
[Stage 0] 0.attn.c_proj.weight -> cuda:0
[Stage 0] 0.ln_2.weight -> cuda:0
[Stage 0] 0.mlp.c_fc.weight -> cuda:0
[Stage 0] 0.mlp.c_proj.weight -> cuda:0
[Stage 0] 1.ln_1.weight -> cuda:0
[Stage 0] 1.attn.c_attn.weight -> cuda:0
[Stage 0] 1.attn.c_proj.weight -> cuda:0
[Stage 0] 1.ln_2.weight -> cuda:0
[Stage 0] 1.mlp.c_fc.weight -> cuda:0
[Stage 0] 1.mlp.c_proj.weight -> cuda:0
[Stage 0] 2.ln_1.weight -> cuda:0
[Stage 0] 2.attn.c_attn.weight -> cuda:0
[Stage 0] 2.attn.c_proj.weight -> cuda:0
[Stage 0] 2.ln_2.weight -> cuda:0
[Stage 0] 2.mlp.c_fc.weight -> cuda:0
[Stage 0] 2.mlp.c_proj.weight -> cuda:0
[Stage 0] 3.ln_1.weight -> cuda:0
[Stage 0] 3.attn.c_attn.weight -> cuda:0
[Stage 0] 3.attn.c_proj.weight -> cuda:0
[Stage 0] 3.ln_2.weight -> cuda:0
[Stage 0] 3.mlp.c_fc.weight -> cuda:0
[Stage 0] 3.mlp.c_proj.weight -> cuda:0
[Stage 0] 4.ln_1.weight -> cuda:0
[Stage 0] 4.attn.c_attn.weight -> cuda:0
[Stage 0] 4.attn.c_proj.weight -> cuda:0
[Stage 0] 4.ln_2.weight -> cuda:0
[Stage 0] 4.mlp.c_fc.weight -> cuda:0
[Stage 0] 4.mlp.c_proj.weight -> cuda:0
[Stage 0] 5.ln_1.weight -> cuda:0
[Stage 0] 5.attn.c_attn.weight -> cuda:0
[Stage 0] 5.attn.c_proj.weight -> cuda:0
[Stage 0] 5.ln_2.weight -> cuda:0
[Stage 0] 5.mlp.c_fc.weight -> cuda:0
[Stage 0] 5.mlp.c_proj.weight -> cuda:0
[Stage 0] 6.ln_1.weight -> cuda:0
[Stage 0] 6.attn.c_attn.weight -> cuda:0
[Stage 0] 6.attn.c_proj.weight -> cuda:0
[Stage 0] 6.ln_2.weight -> cuda:0
[Stage 0] 6.mlp.c_fc.weight -> cuda:0
[Stage 0] 6.mlp.c_proj.weight -> cuda:0
[Stage 0] 7.ln_1.weight -> cuda:0
[Stage 0] 7.attn.c_attn.weight -> cuda:0
[Stage 0] 7.attn.c_proj.weight -> cuda:0
[Stage 0] 7.ln_2.weight -> cuda:0
[Stage 0] 7.mlp.c_fc.weight -> cuda:0
[Stage 0] 7.mlp.c_proj.weight -> cuda:0
[Stage 0] 8.ln_1.weight -> cuda:0
[Stage 0] 8.attn.c_attn.weight -> cuda:0
[Stage 0] 8.attn.c_proj.weight -> cuda:0
[Stage 0] 8.ln_2.weight -> cuda:0
[Stage 0] 8.mlp.c_fc.weight -> cuda:0
[Stage 0] 8.mlp.c_proj.weight -> cuda:0
[Stage 0] 9.ln_1.weight -> cuda:0
[Stage 0] 9.attn.c_attn.weight -> cuda:0
[Stage 0] 9.attn.c_proj.weight -> cuda:0
[Stage 0] 9.ln_2.weight -> cuda:0
[Stage 0] 9.mlp.c_fc.weight -> cuda:0
[Stage 0] 9.mlp.c_proj.weight -> cuda:0
[Stage 0] 10.ln_1.weight -> cuda:0
[Stage 0] 10.attn.c_attn.weight -> cuda:0
[Stage 0] 10.attn.c_proj.weight -> cuda:0
[Stage 0] 10.ln_2.weight -> cuda:0
[Stage 0] 10.mlp.c_fc.weight -> cuda:0
[Stage 0] 10.mlp.c_proj.weight -> cuda:0
[Stage 0] 11.ln_1.weight -> cuda:0
[Stage 0] 11.attn.c_attn.weight -> cuda:0
[Stage 0] 11.attn.c_proj.weight -> cuda:0
[Stage 0] 11.ln_2.weight -> cuda:0
[Stage 0] 11.mlp.c_fc.weight -> cuda:0
[Stage 0] 11.mlp.c_proj.weight -> cuda:0
[Stage 1] 0.ln_1.weight -> cuda:1
[Stage 1] 0.attn.c_attn.weight -> cuda:1
[Stage 1] 0.attn.c_proj.weight -> cuda:1
[Stage 1] 0.ln_2.weight -> cuda:1
[Stage 1] 0.mlp.c_fc.weight -> cuda:1
[Stage 1] 0.mlp.c_proj.weight -> cuda:1
[Stage 1] 1.ln_1.weight -> cuda:1
[Stage 1] 1.attn.c_attn.weight -> cuda:1
[Stage 1] 1.attn.c_proj.weight -> cuda:1
[Stage 1] 1.ln_2.weight -> cuda:1
[Stage 1] 1.mlp.c_fc.weight -> cuda:1
[Stage 1] 1.mlp.c_proj.weight -> cuda:1
[Stage 1] 2.ln_1.weight -> cuda:1
[Stage 1] 2.attn.c_attn.weight -> cuda:1
[Stage 1] 2.attn.c_proj.weight -> cuda:1
[Stage 1] 2.ln_2.weight -> cuda:1
[Stage 1] 2.mlp.c_fc.weight -> cuda:1
[Stage 1] 2.mlp.c_proj.weight -> cuda:1
[Stage 1] 3.ln_1.weight -> cuda:1
[Stage 1] 3.attn.c_attn.weight -> cuda:1
[Stage 1] 3.attn.c_proj.weight -> cuda:1
[Stage 1] 3.ln_2.weight -> cuda:1
[Stage 1] 3.mlp.c_fc.weight -> cuda:1
[Stage 1] 3.mlp.c_proj.weight -> cuda:1
[Stage 1] 4.ln_1.weight -> cuda:1
[Stage 1] 4.attn.c_attn.weight -> cuda:1
[Stage 1] 4.attn.c_proj.weight -> cuda:1
[Stage 1] 4.ln_2.weight -> cuda:1
[Stage 1] 4.mlp.c_fc.weight -> cuda:1
[Stage 1] 4.mlp.c_proj.weight -> cuda:1
[Stage 1] 5.ln_1.weight -> cuda:1
[Stage 1] 5.attn.c_attn.weight -> cuda:1
[Stage 1] 5.attn.c_proj.weight -> cuda:1
[Stage 1] 5.ln_2.weight -> cuda:1
[Stage 1] 5.mlp.c_fc.weight -> cuda:1
[Stage 1] 5.mlp.c_proj.weight -> cuda:1
[Stage 1] 6.ln_1.weight -> cuda:1
[Stage 1] 6.attn.c_attn.weight -> cuda:1
[Stage 1] 6.attn.c_proj.weight -> cuda:1
[Stage 1] 6.ln_2.weight -> cuda:1
[Stage 1] 6.mlp.c_fc.weight -> cuda:1
[Stage 1] 6.mlp.c_proj.weight -> cuda:1
[Stage 1] 7.ln_1.weight -> cuda:1
[Stage 1] 7.attn.c_attn.weight -> cuda:1
[Stage 1] 7.attn.c_proj.weight -> cuda:1
[Stage 1] 7.ln_2.weight -> cuda:1
[Stage 1] 7.mlp.c_fc.weight -> cuda:1
[Stage 1] 7.mlp.c_proj.weight -> cuda:1
[Stage 1] 8.ln_1.weight -> cuda:1
[Stage 1] 8.attn.c_attn.weight -> cuda:1
[Stage 1] 8.attn.c_proj.weight -> cuda:1
[Stage 1] 8.ln_2.weight -> cuda:1
[Stage 1] 8.mlp.c_fc.weight -> cuda:1
[Stage 1] 8.mlp.c_proj.weight -> cuda:1
[Stage 1] 9.ln_1.weight -> cuda:1
[Stage 1] 9.attn.c_attn.weight -> cuda:1
/pscratch/sd/e/es_lh/nanoGPT/MP/train_mp_logger.py:232: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
[Stage 1] 9.attn.c_proj.weight -> cuda:1
[Stage 1] 9.ln_2.weight -> cuda:1
[Stage 1] 9.mlp.c_fc.weight -> cuda:1
[Stage 1] 9.mlp.c_proj.weight -> cuda:1
[Stage 1] 10.ln_1.weight -> cuda:1
[Stage 1] 10.attn.c_attn.weight -> cuda:1
[Stage 1] 10.attn.c_proj.weight -> cuda:1
[Stage 1] 10.ln_2.weight -> cuda:1
[Stage 1] 10.mlp.c_fc.weight -> cuda:1
[Stage 1] 10.mlp.c_proj.weight -> cuda:1
[Stage 1] 11.ln_1.weight -> cuda:1
[Stage 1] 11.attn.c_attn.weight -> cuda:1
[Stage 1] 11.attn.c_proj.weight -> cuda:1
[Stage 1] 11.ln_2.weight -> cuda:1
[Stage 1] 11.mlp.c_fc.weight -> cuda:1
[Stage 1] 11.mlp.c_proj.weight -> cuda:1
[Stage 2] 0.ln_1.weight -> cuda:2
[Stage 2] 0.attn.c_attn.weight -> cuda:2
[Stage 2] 0.attn.c_proj.weight -> cuda:2
[Stage 2] 0.ln_2.weight -> cuda:2
[Stage 2] 0.mlp.c_fc.weight -> cuda:2
[Stage 2] 0.mlp.c_proj.weight -> cuda:2
[Stage 2] 1.ln_1.weight -> cuda:2
[Stage 2] 1.attn.c_attn.weight -> cuda:2
[Stage 2] 1.attn.c_proj.weight -> cuda:2
[Stage 2] 1.ln_2.weight -> cuda:2
[Stage 2] 1.mlp.c_fc.weight -> cuda:2
[Stage 2] 1.mlp.c_proj.weight -> cuda:2
[Stage 2] 2.ln_1.weight -> cuda:2
[Stage 2] 2.attn.c_attn.weight -> cuda:2
[Stage 2] 2.attn.c_proj.weight -> cuda:2
[Stage 2] 2.ln_2.weight -> cuda:2
[Stage 2] 2.mlp.c_fc.weight -> cuda:2
[Stage 2] 2.mlp.c_proj.weight -> cuda:2
[Stage 2] 3.ln_1.weight -> cuda:2
[Stage 2] 3.attn.c_attn.weight -> cuda:2
[Stage 2] 3.attn.c_proj.weight -> cuda:2
[Stage 2] 3.ln_2.weight -> cuda:2
[Stage 2] 3.mlp.c_fc.weight -> cuda:2
[Stage 2] 3.mlp.c_proj.weight -> cuda:2
[Stage 2] 4.ln_1.weight -> cuda:2
[Stage 2] 4.attn.c_attn.weight -> cuda:2
[Stage 2] 4.attn.c_proj.weight -> cuda:2
[Stage 2] 4.ln_2.weight -> cuda:2
[Stage 2] 4.mlp.c_fc.weight -> cuda:2
[Stage 2] 4.mlp.c_proj.weight -> cuda:2
[Stage 2] 5.ln_1.weight -> cuda:2
[Stage 2] 5.attn.c_attn.weight -> cuda:2
[Stage 2] 5.attn.c_proj.weight -> cuda:2
[Stage 2] 5.ln_2.weight -> cuda:2
[Stage 2] 5.mlp.c_fc.weight -> cuda:2
[Stage 2] 5.mlp.c_proj.weight -> cuda:2
[Stage 2] 6.ln_1.weight -> cuda:2
[Stage 2] 6.attn.c_attn.weight -> cuda:2
[Stage 2] 6.attn.c_proj.weight -> cuda:2
[Stage 2] 6.ln_2.weight -> cuda:2
[Stage 2] 6.mlp.c_fc.weight -> cuda:2
[Stage 2] 6.mlp.c_proj.weight -> cuda:2
[Stage 2] 7.ln_1.weight -> cuda:2
[Stage 2] 7.attn.c_attn.weight -> cuda:2
[Stage 2] 7.attn.c_proj.weight -> cuda:2
[Stage 2] 7.ln_2.weight -> cuda:2
[Stage 2] 7.mlp.c_fc.weight -> cuda:2
[Stage 2] 7.mlp.c_proj.weight -> cuda:2
[Stage 2] 8.ln_1.weight -> cuda:2
[Stage 2] 8.attn.c_attn.weight -> cuda:2
[Stage 2] 8.attn.c_proj.weight -> cuda:2
[Stage 2] 8.ln_2.weight -> cuda:2
[Stage 2] 8.mlp.c_fc.weight -> cuda:2
[Stage 2] 8.mlp.c_proj.weight -> cuda:2
[Stage 2] 9.ln_1.weight -> cuda:2
[Stage 2] 9.attn.c_attn.weight -> cuda:2
[Stage 2] 9.attn.c_proj.weight -> cuda:2
[Stage 2] 9.ln_2.weight -> cuda:2
[Stage 2] 9.mlp.c_fc.weight -> cuda:2
[Stage 2] 9.mlp.c_proj.weight -> cuda:2
[Stage 2] 10.ln_1.weight -> cuda:2
[Stage 2] 10.attn.c_attn.weight -> cuda:2
[Stage 2] 10.attn.c_proj.weight -> cuda:2
[Stage 2] 10.ln_2.weight -> cuda:2
[Stage 2] 10.mlp.c_fc.weight -> cuda:2
[Stage 2] 10.mlp.c_proj.weight -> cuda:2
[Stage 2] 11.ln_1.weight -> cuda:2
[Stage 2] 11.attn.c_attn.weight -> cuda:2
[Stage 2] 11.attn.c_proj.weight -> cuda:2
[Stage 2] 11.ln_2.weight -> cuda:2
[Stage 2] 11.mlp.c_fc.weight -> cuda:2
[Stage 2] 11.mlp.c_proj.weight -> cuda:2
[Stage 3] 0.ln_1.weight -> cuda:3
[Stage 3] 0.attn.c_attn.weight -> cuda:3
[Stage 3] 0.attn.c_proj.weight -> cuda:3
[Stage 3] 0.ln_2.weight -> cuda:3
[Stage 3] 0.mlp.c_fc.weight -> cuda:3
[Stage 3] 0.mlp.c_proj.weight -> cuda:3
[Stage 3] 1.ln_1.weight -> cuda:3
[Stage 3] 1.attn.c_attn.weight -> cuda:3
[Stage 3] 1.attn.c_proj.weight -> cuda:3
[Stage 3] 1.ln_2.weight -> cuda:3
[Stage 3] 1.mlp.c_fc.weight -> cuda:3
[Stage 3] 1.mlp.c_proj.weight -> cuda:3
[Stage 3] 2.ln_1.weight -> cuda:3
[Stage 3] 2.attn.c_attn.weight -> cuda:3
[Stage 3] 2.attn.c_proj.weight -> cuda:3
[Stage 3] 2.ln_2.weight -> cuda:3
[Stage 3] 2.mlp.c_fc.weight -> cuda:3
[Stage 3] 2.mlp.c_proj.weight -> cuda:3
[Stage 3] 3.ln_1.weight -> cuda:3
[Stage 3] 3.attn.c_attn.weight -> cuda:3
[Stage 3] 3.attn.c_proj.weight -> cuda:3
[Stage 3] 3.ln_2.weight -> cuda:3
[Stage 3] 3.mlp.c_fc.weight -> cuda:3
[Stage 3] 3.mlp.c_proj.weight -> cuda:3
[Stage 3] 4.ln_1.weight -> cuda:3
[Stage 3] 4.attn.c_attn.weight -> cuda:3
[Stage 3] 4.attn.c_proj.weight -> cuda:3
[Stage 3] 4.ln_2.weight -> cuda:3
[Stage 3] 4.mlp.c_fc.weight -> cuda:3
[Stage 3] 4.mlp.c_proj.weight -> cuda:3
[Stage 3] 5.ln_1.weight -> cuda:3
[Stage 3] 5.attn.c_attn.weight -> cuda:3
[Stage 3] 5.attn.c_proj.weight -> cuda:3
[Stage 3] 5.ln_2.weight -> cuda:3
[Stage 3] 5.mlp.c_fc.weight -> cuda:3
[Stage 3] 5.mlp.c_proj.weight -> cuda:3
[Stage 3] 6.ln_1.weight -> cuda:3
[Stage 3] 6.attn.c_attn.weight -> cuda:3
[Stage 3] 6.attn.c_proj.weight -> cuda:3
[Stage 3] 6.ln_2.weight -> cuda:3
[Stage 3] 6.mlp.c_fc.weight -> cuda:3
[Stage 3] 6.mlp.c_proj.weight -> cuda:3
[Stage 3] 7.ln_1.weight -> cuda:3
[Stage 3] 7.attn.c_attn.weight -> cuda:3
[Stage 3] 7.attn.c_proj.weight -> cuda:3
[Stage 3] 7.ln_2.weight -> cuda:3
[Stage 3] 7.mlp.c_fc.weight -> cuda:3
[Stage 3] 7.mlp.c_proj.weight -> cuda:3
[Stage 3] 8.ln_1.weight -> cuda:3
[Stage 3] 8.attn.c_attn.weight -> cuda:3
[Stage 3] 8.attn.c_proj.weight -> cuda:3
[Stage 3] 8.ln_2.weight -> cuda:3
[Stage 3] 8.mlp.c_fc.weight -> cuda:3
[Stage 3] 8.mlp.c_proj.weight -> cuda:3
[Stage 3] 9.ln_1.weight -> cuda:3
[Stage 3] 9.attn.c_attn.weight -> cuda:3
[Stage 3] 9.attn.c_proj.weight -> cuda:3
[Stage 3] 9.ln_2.weight -> cuda:3
[Stage 3] 9.mlp.c_fc.weight -> cuda:3
[Stage 3] 9.mlp.c_proj.weight -> cuda:3
[Stage 3] 10.ln_1.weight -> cuda:3
[Stage 3] 10.attn.c_attn.weight -> cuda:3
[Stage 3] 10.attn.c_proj.weight -> cuda:3
[Stage 3] 10.ln_2.weight -> cuda:3
[Stage 3] 10.mlp.c_fc.weight -> cuda:3
[Stage 3] 10.mlp.c_proj.weight -> cuda:3
[Stage 3] 11.ln_1.weight -> cuda:3
[Stage 3] 11.attn.c_attn.weight -> cuda:3
[Stage 3] 11.attn.c_proj.weight -> cuda:3
[Stage 3] 11.ln_2.weight -> cuda:3
[Stage 3] 11.mlp.c_fc.weight -> cuda:3
[Stage 3] 11.mlp.c_proj.weight -> cuda:3
num decayed parameter tensors: 195, with 2,416,709,632 parameters
num non-decayed parameter tensors: 97, with 198,656 parameters
using fused AdamW: True
[RunLogger] Model params: 2,416,908,288 (2416.91 M), weights ~ 9.004 GB, full train state ~ 36.015 GB (theoretical)
step 0: train loss 4.5928, val loss 4.5904
iter 0: loss 4.6010, time 62049.48ms, mfu -100.00%
iter 10: loss 3.6116, time 1616.41ms, mfu 48.08%
iter 20: loss 3.4086, time 1633.34ms, mfu 48.03%
iter 30: loss 2.8704, time 1621.58ms, mfu 48.02%
iter 40: loss 2.7012, time 1603.74ms, mfu 48.07%
iter 50: loss 2.5848, time 1657.52ms, mfu 47.95%
iter 60: loss 2.5663, time 1587.84ms, mfu 48.05%
iter 70: loss 2.5483, time 1536.10ms, mfu 48.30%
iter 80: loss 2.6061, time 1640.96ms, mfu 48.21%
iter 90: loss 2.5751, time 1635.53ms, mfu 48.14%
iter 100: loss 2.5050, time 1584.18ms, mfu 48.23%
iter 110: loss 2.5112, time 1617.44ms, mfu 48.21%
iter 120: loss 2.5355, time 1542.34ms, mfu 48.43%
iter 130: loss 2.4429, time 1572.34ms, mfu 48.53%
iter 140: loss 2.4981, time 1669.15ms, mfu 48.33%
iter 150: loss 2.4503, time 1603.10ms, mfu 48.35%
iter 160: loss 2.4181, time 1535.65ms, mfu 48.58%
iter 170: loss 2.3720, time 1584.22ms, mfu 48.62%
iter 180: loss 2.3453, time 1668.76ms, mfu 48.42%
iter 190: loss 2.3222, time 1588.09ms, mfu 48.47%
iter 200: loss 2.3157, time 1569.42ms, mfu 48.58%
iter 210: loss 2.2458, time 1572.05ms, mfu 48.66%
iter 220: loss 2.2611, time 1717.07ms, mfu 48.32%
iter 230: loss 2.1796, time 1598.72ms, mfu 48.35%
iter 240: loss 2.1657, time 1573.10ms, mfu 48.46%
step 250: train loss 2.0997, val loss 2.1775
saving checkpoint to out-shakespeare-char
iter 250: loss 2.1524, time 101129.78ms, mfu 43.69%
iter 260: loss 2.0577, time 1605.87ms, mfu 44.16%
iter 270: loss 2.1535, time 1626.55ms, mfu 44.52%
iter 280: loss 2.0523, time 1624.57ms, mfu 44.85%
iter 290: loss 2.0002, time 1606.16ms, mfu 45.21%
iter 300: loss 1.9819, time 1626.30ms, mfu 45.47%
iter 310: loss 2.0053, time 1606.05ms, mfu 45.76%
iter 320: loss 1.9818, time 1624.99ms, mfu 45.96%
iter 330: loss 1.9271, time 1628.21ms, mfu 46.14%
iter 340: loss 1.9073, time 1606.47ms, mfu 46.37%
iter 350: loss 1.9054, time 1605.88ms, mfu 46.57%
iter 360: loss 1.8900, time 1540.73ms, mfu 46.96%
iter 370: loss 1.8428, time 1606.39ms, mfu 47.10%
iter 380: loss 1.8620, time 1589.81ms, mfu 47.28%
iter 390: loss 1.8038, time 1531.45ms, mfu 47.62%
iter 400: loss 1.8129, time 1567.76ms, mfu 47.82%
iter 410: loss 1.8065, time 1623.75ms, mfu 47.82%
iter 420: loss 1.8512, time 1629.60ms, mfu 47.81%
iter 430: loss 1.8171, time 1605.83ms, mfu 47.87%
iter 440: loss 1.7959, time 1605.63ms, mfu 47.92%
iter 450: loss 1.7796, time 1630.42ms, mfu 47.90%
iter 460: loss 1.7454, time 1605.91ms, mfu 47.95%
iter 470: loss 1.7139, time 1606.05ms, mfu 47.99%
iter 480: loss 1.7399, time 1605.82ms, mfu 48.03%
iter 490: loss 1.7111, time 1628.45ms, mfu 48.00%
step 500: train loss 1.6347, val loss 1.8457
saving checkpoint to out-shakespeare-char
iter 500: loss 1.7022, time 106596.35ms, mfu 43.28%
iter 510: loss 1.6582, time 1687.21ms, mfu 43.55%
iter 520: loss 1.6046, time 1597.30ms, mfu 44.06%
iter 530: loss 1.6718, time 1640.89ms, mfu 44.39%
iter 540: loss 1.6438, time 1595.66ms, mfu 44.83%
iter 550: loss 1.6473, time 1668.41ms, mfu 45.00%
iter 560: loss 1.6481, time 1571.48ms, mfu 45.45%
iter 570: loss 1.5464, time 1616.02ms, mfu 45.71%
iter 580: loss 1.5594, time 1608.14ms, mfu 45.97%
iter 590: loss 1.5544, time 1570.45ms, mfu 46.33%
iter 600: loss 1.6346, time 1646.60ms, mfu 46.41%
iter 610: loss 1.5814, time 1577.67ms, mfu 46.70%
iter 620: loss 1.5457, time 1649.74ms, mfu 46.74%
iter 630: loss 1.5123, time 1633.58ms, mfu 46.82%
iter 640: loss 1.4889, time 1592.94ms, mfu 47.02%
iter 650: loss 1.5095, time 1600.09ms, mfu 47.17%
iter 660: loss 1.4858, time 1600.04ms, mfu 47.31%
iter 670: loss 1.5131, time 1606.14ms, mfu 47.42%
iter 680: loss 1.5050, time 1643.10ms, mfu 47.41%
iter 690: loss 1.4624, time 1565.32ms, mfu 47.63%
iter 700: loss 1.4890, time 1588.95ms, mfu 47.76%
iter 710: loss 1.4739, time 1668.53ms, mfu 47.64%
iter 720: loss 1.4523, time 1587.42ms, mfu 47.78%
iter 730: loss 1.4596, time 1605.86ms, mfu 47.84%
iter 740: loss 1.4503, time 1605.96ms, mfu 47.89%
step 750: train loss 1.3848, val loss 1.6649
saving checkpoint to out-shakespeare-char
iter 750: loss 1.3933, time 106786.08ms, mfu 43.18%
iter 760: loss 1.4107, time 1627.31ms, mfu 43.64%
iter 770: loss 1.4811, time 1626.35ms, mfu 44.05%
iter 780: loss 1.4452, time 1605.90ms, mfu 44.49%
iter 790: loss 1.5102, time 1625.41ms, mfu 44.82%
iter 800: loss 1.4155, time 1606.37ms, mfu 45.17%
iter 810: loss 1.3540, time 1605.92ms, mfu 45.50%
iter 820: loss 1.3574, time 1606.19ms, mfu 45.79%
iter 830: loss 1.3132, time 1606.57ms, mfu 46.05%
iter 840: loss 1.3988, time 1605.93ms, mfu 46.28%
iter 850: loss 1.3722, time 1605.86ms, mfu 46.49%
iter 860: loss 1.3602, time 1605.86ms, mfu 46.68%
iter 870: loss 1.3594, time 1629.29ms, mfu 46.78%
iter 880: loss 1.2376, time 1606.41ms, mfu 46.94%
iter 890: loss 1.3263, time 1601.03ms, mfu 47.10%
iter 900: loss 1.2446, time 1642.63ms, mfu 47.13%
iter 910: loss 1.3951, time 1653.31ms, mfu 47.11%
iter 920: loss 1.2903, time 1611.73ms, mfu 47.22%
iter 930: loss 1.2746, time 1582.55ms, mfu 47.41%
iter 940: loss 1.2575, time 1682.66ms, mfu 47.29%
iter 950: loss 1.3219, time 1583.03ms, mfu 47.47%
iter 960: loss 1.3153, time 1673.11ms, mfu 47.37%
iter 970: loss 1.2403, time 1568.99ms, mfu 47.59%
iter 980: loss 1.3388, time 1712.73ms, mfu 47.37%
iter 990: loss 1.3095, time 1536.29ms, mfu 47.69%
step 1000: train loss 1.2046, val loss 1.5978
saving checkpoint to out-shakespeare-char
iter 1000: loss 1.2992, time 107116.88ms, mfu 42.99%
iter 1010: loss 1.2064, time 1607.17ms, mfu 43.53%
iter 1020: loss 1.2845, time 1606.59ms, mfu 44.01%
iter 1030: loss 1.2906, time 1633.44ms, mfu 44.37%
iter 1040: loss 1.1843, time 1603.71ms, mfu 44.78%
iter 1050: loss 1.2261, time 1594.10ms, mfu 45.18%
iter 1060: loss 1.2489, time 1671.61ms, mfu 45.31%
iter 1070: loss 1.2196, time 1634.39ms, mfu 45.53%
iter 1080: loss 1.1750, time 1571.96ms, mfu 45.92%
iter 1090: loss 1.1759, time 1573.92ms, mfu 46.27%
iter 1100: loss 1.2091, time 1572.62ms, mfu 46.58%
iter 1110: loss 1.1661, time 1705.47ms, mfu 46.48%
iter 1120: loss 1.1942, time 1572.80ms, mfu 46.78%
iter 1130: loss 1.2335, time 1645.80ms, mfu 46.82%
iter 1140: loss 1.2011, time 1606.05ms, mfu 46.98%
iter 1150: loss 1.1262, time 1605.91ms, mfu 47.12%
iter 1160: loss 1.1632, time 1606.17ms, mfu 47.25%
iter 1170: loss 1.1327, time 1671.24ms, mfu 47.17%
iter 1180: loss 1.1064, time 1595.82ms, mfu 47.33%
iter 1190: loss 1.2176, time 1577.05ms, mfu 47.52%
iter 1200: loss 1.0834, time 1692.57ms, mfu 47.36%
iter 1210: loss 1.0948, time 1610.80ms, mfu 47.45%
iter 1220: loss 1.1512, time 1660.83ms, mfu 47.38%
iter 1230: loss 1.0841, time 1668.77ms, mfu 47.30%
iter 1240: loss 1.0287, time 1607.75ms, mfu 47.41%
step 1250: train loss 0.9870, val loss 1.6442
iter 1250: loss 1.0731, time 60905.95ms, mfu 42.79%
iter 1260: loss 1.0957, time 1630.35ms, mfu 43.28%
iter 1270: loss 1.0609, time 1634.90ms, mfu 43.71%
iter 1280: loss 1.0436, time 1606.63ms, mfu 44.17%
iter 1290: loss 1.0573, time 1605.95ms, mfu 44.60%
iter 1300: loss 1.0495, time 1605.95ms, mfu 44.98%
iter 1310: loss 1.0341, time 1606.01ms, mfu 45.32%
iter 1320: loss 1.0014, time 1630.59ms, mfu 45.55%
iter 1330: loss 0.9734, time 1627.80ms, mfu 45.77%
iter 1340: loss 1.0010, time 1631.18ms, mfu 45.96%
iter 1350: loss 0.9528, time 1627.60ms, mfu 46.14%
iter 1360: loss 1.0217, time 1639.65ms, mfu 46.26%
iter 1370: loss 0.9698, time 1606.19ms, mfu 46.48%
iter 1380: loss 0.9719, time 1606.12ms, mfu 46.67%
iter 1390: loss 0.9126, time 1606.06ms, mfu 46.84%
iter 1400: loss 0.9833, time 1606.13ms, mfu 47.00%
iter 1410: loss 0.9682, time 1606.16ms, mfu 47.14%
iter 1420: loss 0.9169, time 1626.23ms, mfu 47.20%
iter 1430: loss 0.9625, time 1606.46ms, mfu 47.32%
iter 1440: loss 0.9178, time 1606.22ms, mfu 47.43%
iter 1450: loss 0.9027, time 1634.72ms, mfu 47.44%
iter 1460: loss 0.8697, time 1605.82ms, mfu 47.53%
iter 1470: loss 0.9494, time 1621.36ms, mfu 47.57%
iter 1480: loss 0.8400, time 1606.24ms, mfu 47.66%
iter 1490: loss 0.8343, time 1606.15ms, mfu 47.73%
step 1500: train loss 0.7123, val loss 1.7961
iter 1500: loss 0.8824, time 60932.00ms, mfu 43.08%
iter 1510: loss 0.7756, time 1605.96ms, mfu 43.61%
iter 1520: loss 0.8730, time 1606.15ms, mfu 44.09%
iter 1530: loss 0.8989, time 1634.01ms, mfu 44.44%
iter 1540: loss 0.7947, time 1606.03ms, mfu 44.83%
iter 1550: loss 0.7155, time 1606.27ms, mfu 45.19%
iter 1560: loss 0.8050, time 1605.95ms, mfu 45.51%
iter 1570: loss 0.8425, time 1628.33ms, mfu 45.73%
iter 1580: loss 0.7344, time 1606.04ms, mfu 46.00%
iter 1590: loss 0.7184, time 1635.23ms, mfu 46.15%
iter 1600: loss 0.7691, time 1621.52ms, mfu 46.33%
iter 1610: loss 0.7236, time 1610.95ms, mfu 46.52%
iter 1620: loss 0.6300, time 1594.59ms, mfu 46.74%
iter 1630: loss 0.7355, time 1593.34ms, mfu 46.95%
iter 1640: loss 0.7736, time 1598.45ms, mfu 47.11%
iter 1650: loss 0.6447, time 1585.74ms, mfu 47.30%
iter 1660: loss 0.6239, time 1668.92ms, mfu 47.23%
iter 1670: loss 0.6955, time 1565.98ms, mfu 47.47%
iter 1680: loss 0.6423, time 1670.27ms, mfu 47.38%
iter 1690: loss 0.6801, time 1689.17ms, mfu 47.24%
iter 1700: loss 0.6507, time 1690.63ms, mfu 47.11%
iter 1710: loss 0.5988, time 1592.25ms, mfu 47.28%
iter 1720: loss 0.6261, time 1567.43ms, mfu 47.51%
iter 1730: loss 0.5978, time 1668.39ms, mfu 47.42%
iter 1740: loss 0.5637, time 1572.54ms, mfu 47.62%
step 1750: train loss 0.4169, val loss 2.0716
iter 1750: loss 0.6467, time 60831.21ms, mfu 42.99%
iter 1760: loss 0.5845, time 1606.16ms, mfu 43.53%
iter 1770: loss 0.6761, time 1632.80ms, mfu 43.93%
iter 1780: loss 0.5862, time 1629.39ms, mfu 44.31%
iter 1790: loss 0.5622, time 1624.97ms, mfu 44.66%
iter 1800: loss 0.5243, time 1606.18ms, mfu 45.03%
iter 1810: loss 0.4747, time 1606.27ms, mfu 45.37%
iter 1820: loss 0.5361, time 1605.68ms, mfu 45.67%
iter 1830: loss 0.5136, time 1630.17ms, mfu 45.87%
iter 1840: loss 0.6467, time 1606.09ms, mfu 46.13%
iter 1850: loss 0.5296, time 1606.25ms, mfu 46.35%
iter 1860: loss 0.4463, time 1606.29ms, mfu 46.55%
iter 1870: loss 0.4476, time 1606.31ms, mfu 46.74%
iter 1880: loss 0.4616, time 1629.29ms, mfu 46.83%
iter 1890: loss 0.4736, time 1606.14ms, mfu 46.99%
iter 1900: loss 0.4917, time 1578.43ms, mfu 47.21%
iter 1910: loss 0.4777, time 1595.92ms, mfu 47.36%
iter 1920: loss 0.5239, time 1642.33ms, mfu 47.36%
iter 1930: loss 0.4456, time 1664.55ms, mfu 47.29%
iter 1940: loss 0.4188, time 1710.47ms, mfu 47.11%
iter 1950: loss 0.4413, time 1568.27ms, mfu 47.35%
iter 1960: loss 0.4263, time 1668.75ms, mfu 47.27%
iter 1970: loss 0.4408, time 1574.83ms, mfu 47.48%
iter 1980: loss 0.3763, time 1704.75ms, mfu 47.29%
iter 1990: loss 0.4625, time 1620.51ms, mfu 47.36%
step 2000: train loss 0.2297, val loss 2.4089
iter 2000: loss 0.3300, time 61033.11ms, mfu 42.75%
iter 2010: loss 0.3804, time 1611.54ms, mfu 43.30%
iter 2020: loss 0.3412, time 1605.85ms, mfu 43.81%
iter 2030: loss 0.3482, time 1632.32ms, mfu 44.19%
iter 2040: loss 0.3988, time 1605.92ms, mfu 44.61%
iter 2050: loss 0.3438, time 1634.12ms, mfu 44.90%
iter 2060: loss 0.3981, time 1621.70ms, mfu 45.21%
iter 2070: loss 0.3745, time 1619.99ms, mfu 45.48%
iter 2080: loss 0.2910, time 1555.41ms, mfu 45.93%
iter 2090: loss 0.3333, time 1695.82ms, mfu 45.92%
iter 2100: loss 0.3361, time 1576.67ms, mfu 46.26%
iter 2110: loss 0.3284, time 1591.63ms, mfu 46.52%
iter 2120: loss 0.2969, time 1703.69ms, mfu 46.43%
iter 2130: loss 0.2968, time 1603.96ms, mfu 46.63%
iter 2140: loss 0.3265, time 1581.06ms, mfu 46.88%
iter 2150: loss 0.2968, time 1572.94ms, mfu 47.14%
iter 2160: loss 0.3042, time 1680.08ms, mfu 47.05%
iter 2170: loss 0.2589, time 1616.77ms, mfu 47.15%
iter 2180: loss 0.3099, time 1627.31ms, mfu 47.21%
iter 2190: loss 0.2512, time 1605.78ms, mfu 47.33%
iter 2200: loss 0.3066, time 1606.24ms, mfu 47.44%
iter 2210: loss 0.2970, time 1630.61ms, mfu 47.46%
iter 2220: loss 0.3382, time 1629.80ms, mfu 47.48%
iter 2230: loss 0.2505, time 1605.45ms, mfu 47.57%
iter 2240: loss 0.3001, time 1606.07ms, mfu 47.66%
step 2250: train loss 0.1583, val loss 2.6709
iter 2250: loss 0.2863, time 60930.63ms, mfu 43.02%
iter 2260: loss 0.2691, time 1686.69ms, mfu 43.32%
iter 2270: loss 0.2956, time 1581.05ms, mfu 43.91%
iter 2280: loss 0.2428, time 1637.18ms, mfu 44.26%
iter 2290: loss 0.3066, time 1626.70ms, mfu 44.62%
iter 2300: loss 0.2727, time 1605.45ms, mfu 44.99%
iter 2310: loss 0.2392, time 1605.92ms, mfu 45.33%
iter 2320: loss 0.2511, time 1630.47ms, mfu 45.57%
iter 2330: loss 0.2465, time 1606.35ms, mfu 45.85%
iter 2340: loss 0.2615, time 1628.32ms, mfu 46.04%
iter 2350: loss 0.2526, time 1605.95ms, mfu 46.27%
iter 2360: loss 0.2366, time 1580.25ms, mfu 46.56%
iter 2370: loss 0.2492, time 1588.09ms, mfu 46.80%
iter 2380: loss 0.2317, time 1548.37ms, mfu 47.14%
iter 2390: loss 0.2501, time 1585.12ms, mfu 47.33%
iter 2400: loss 0.2521, time 1585.78ms, mfu 47.50%
iter 2410: loss 0.2627, time 1573.52ms, mfu 47.69%
iter 2420: loss 0.2167, time 1688.80ms, mfu 47.52%
iter 2430: loss 0.2434, time 1532.19ms, mfu 47.84%
iter 2440: loss 0.2238, time 1683.27ms, mfu 47.67%
iter 2450: loss 0.2191, time 1693.50ms, mfu 47.50%
iter 2460: loss 0.2111, time 1561.04ms, mfu 47.73%
iter 2470: loss 0.2237, time 1584.44ms, mfu 47.86%
iter 2480: loss 0.2372, time 1535.63ms, mfu 48.13%
iter 2490: loss 0.2265, time 1669.18ms, mfu 47.98%
step 2500: train loss 0.1312, val loss 2.8727
iter 2500: loss 0.2048, time 60955.06ms, mfu 43.31%
iter 2510: loss 0.2239, time 1623.26ms, mfu 43.76%
iter 2520: loss 0.2148, time 1605.85ms, mfu 44.23%
iter 2530: loss 0.2004, time 1606.36ms, mfu 44.64%
iter 2540: loss 0.2086, time 1629.39ms, mfu 44.95%
iter 2550: loss 0.2179, time 1606.26ms, mfu 45.29%
iter 2560: loss 0.2072, time 1614.79ms, mfu 45.58%
iter 2570: loss 0.2156, time 1591.68ms, mfu 45.90%
iter 2580: loss 0.2170, time 1658.14ms, mfu 46.00%
iter 2590: loss 0.2001, time 1632.66ms, mfu 46.16%
iter 2600: loss 0.1855, time 1634.94ms, mfu 46.30%
iter 2610: loss 0.2144, time 1591.12ms, mfu 46.55%
iter 2620: loss 0.2035, time 1698.72ms, mfu 46.47%
iter 2630: loss 0.1875, time 1605.73ms, mfu 46.66%
iter 2640: loss 0.1891, time 1605.60ms, mfu 46.84%
iter 2650: loss 0.1965, time 1623.89ms, mfu 46.94%
iter 2660: loss 0.2249, time 1605.89ms, mfu 47.09%
iter 2670: loss 0.1783, time 1625.15ms, mfu 47.16%
iter 2680: loss 0.1935, time 1591.07ms, mfu 47.33%
iter 2690: loss 0.1712, time 1597.16ms, mfu 47.46%
iter 2700: loss 0.1691, time 1657.39ms, mfu 47.41%
iter 2710: loss 0.1859, time 1585.63ms, mfu 47.57%
iter 2720: loss 0.1919, time 1628.47ms, mfu 47.58%
iter 2730: loss 0.1918, time 1529.56ms, mfu 47.91%
iter 2740: loss 0.1683, time 1568.47ms, mfu 48.07%
step 2750: train loss 0.1148, val loss 3.0818
iter 2750: loss 0.1949, time 61093.06ms, mfu 43.39%
iter 2760: loss 0.1934, time 1613.31ms, mfu 43.87%
iter 2770: loss 0.1813, time 1586.19ms, mfu 44.38%
iter 2780: loss 0.1844, time 1596.10ms, mfu 44.81%
iter 2790: loss 0.1701, time 1637.03ms, mfu 45.08%
iter 2800: loss 0.1691, time 1715.24ms, mfu 45.10%
iter 2810: loss 0.1670, time 1587.38ms, mfu 45.49%
iter 2820: loss 0.1881, time 1707.87ms, mfu 45.49%
iter 2830: loss 0.1817, time 1696.85ms, mfu 45.52%
iter 2840: loss 0.1875, time 1590.62ms, mfu 45.86%
iter 2850: loss 0.1596, time 1690.43ms, mfu 45.87%
iter 2860: loss 0.1654, time 1576.29ms, mfu 46.21%
iter 2870: loss 0.1606, time 1642.81ms, mfu 46.32%
iter 2880: loss 0.1905, time 1670.23ms, mfu 46.34%
iter 2890: loss 0.1527, time 1590.55ms, mfu 46.59%
iter 2900: loss 0.1759, time 1572.81ms, mfu 46.88%
iter 2910: loss 0.1641, time 1592.87ms, mfu 47.07%
iter 2920: loss 0.1489, time 1654.20ms, mfu 47.06%
iter 2930: loss 0.1741, time 1574.25ms, mfu 47.29%
iter 2940: loss 0.1555, time 1567.60ms, mfu 47.52%
iter 2950: loss 0.1656, time 1663.94ms, mfu 47.44%
iter 2960: loss 0.1671, time 1602.51ms, mfu 47.54%
iter 2970: loss 0.1529, time 1621.90ms, mfu 47.58%
iter 2980: loss 0.1686, time 1665.62ms, mfu 47.49%
iter 2990: loss 0.1647, time 1633.63ms, mfu 47.50%
step 3000: train loss 0.1048, val loss 3.2487
iter 3000: loss 0.1623, time 60925.72ms, mfu 42.88%
iter 3010: loss 0.1559, time 1636.01ms, mfu 43.34%
iter 3020: loss 0.1761, time 1628.75ms, mfu 43.78%
iter 3030: loss 0.1382, time 1611.18ms, mfu 44.22%
iter 3040: loss 0.1576, time 1592.16ms, mfu 44.68%
iter 3050: loss 0.1417, time 1659.50ms, mfu 44.90%
iter 3060: loss 0.1418, time 1695.94ms, mfu 44.99%
iter 3070: loss 0.1600, time 1672.63ms, mfu 45.14%
iter 3080: loss 0.1554, time 1525.31ms, mfu 45.72%
iter 3090: loss 0.1465, time 1681.55ms, mfu 45.77%
iter 3100: loss 0.1578, time 1567.22ms, mfu 46.15%
iter 3110: loss 0.1583, time 1668.14ms, mfu 46.20%
iter 3120: loss 0.1505, time 1610.73ms, mfu 46.40%
iter 3130: loss 0.1502, time 1589.27ms, mfu 46.65%
iter 3140: loss 0.1546, time 1570.34ms, mfu 46.94%
iter 3150: loss 0.1655, time 1668.17ms, mfu 46.90%
iter 3160: loss 0.1354, time 1677.94ms, mfu 46.84%
iter 3170: loss 0.1546, time 1578.39ms, mfu 47.08%
iter 3180: loss 0.1432, time 1587.38ms, mfu 47.27%
iter 3190: loss 0.1478, time 1702.21ms, mfu 47.11%
iter 3200: loss 0.1431, time 1687.10ms, mfu 47.01%
iter 3210: loss 0.1372, time 1689.69ms, mfu 46.90%
iter 3220: loss 0.1494, time 1541.27ms, mfu 47.26%
iter 3230: loss 0.1234, time 1627.62ms, mfu 47.31%
iter 3240: loss 0.1455, time 1673.11ms, mfu 47.22%
step 3250: train loss 0.0976, val loss 3.3656
iter 3250: loss 0.1462, time 60983.22ms, mfu 42.63%
iter 3260: loss 0.1449, time 1586.55ms, mfu 43.26%
iter 3270: loss 0.1309, time 1572.70ms, mfu 43.88%
iter 3280: loss 0.1525, time 1570.43ms, mfu 44.44%
iter 3290: loss 0.1433, time 1548.43ms, mfu 45.01%
iter 3300: loss 0.1495, time 1575.36ms, mfu 45.45%
iter 3310: loss 0.1410, time 1546.31ms, mfu 45.93%
iter 3320: loss 0.1324, time 1588.35ms, mfu 46.23%
iter 3330: loss 0.1215, time 1581.84ms, mfu 46.52%
iter 3340: loss 0.1352, time 1592.46ms, mfu 46.75%
iter 3350: loss 0.1453, time 1682.39ms, mfu 46.69%
iter 3360: loss 0.1362, time 1541.19ms, mfu 47.07%
iter 3370: loss 0.1365, time 1592.93ms, mfu 47.24%
iter 3380: loss 0.1403, time 1603.72ms, mfu 47.36%
iter 3390: loss 0.1348, time 1588.87ms, mfu 47.52%
iter 3400: loss 0.1395, time 1687.06ms, mfu 47.37%
iter 3410: loss 0.1385, time 1542.82ms, mfu 47.67%
iter 3420: loss 0.1362, time 1526.46ms, mfu 48.00%
iter 3430: loss 0.1261, time 1552.29ms, mfu 48.20%
iter 3440: loss 0.1246, time 1671.71ms, mfu 48.03%
iter 3450: loss 0.1460, time 1546.87ms, mfu 48.25%
iter 3460: loss 0.1187, time 1597.45ms, mfu 48.29%
iter 3470: loss 0.1359, time 1584.41ms, mfu 48.37%
iter 3480: loss 0.1182, time 1583.26ms, mfu 48.44%
iter 3490: loss 0.1292, time 1670.83ms, mfu 48.25%
step 3500: train loss 0.0918, val loss 3.5614
iter 3500: loss 0.1341, time 60824.60ms, mfu 43.55%
iter 3510: loss 0.1269, time 1605.74ms, mfu 44.04%
iter 3520: loss 0.1320, time 1626.64ms, mfu 44.41%
iter 3530: loss 0.1197, time 1606.22ms, mfu 44.81%
iter 3540: loss 0.1410, time 1606.22ms, mfu 45.17%
iter 3550: loss 0.1278, time 1605.72ms, mfu 45.49%
iter 3560: loss 0.1168, time 1606.30ms, mfu 45.78%
iter 3570: loss 0.1181, time 1634.57ms, mfu 45.96%
iter 3580: loss 0.1213, time 1648.34ms, mfu 46.08%
iter 3590: loss 0.1326, time 1567.61ms, mfu 46.43%
iter 3600: loss 0.1309, time 1694.71ms, mfu 46.37%
iter 3610: loss 0.1255, time 1672.38ms, mfu 46.38%
iter 3620: loss 0.1232, time 1527.68ms, mfu 46.83%
iter 3630: loss 0.1024, time 1695.10ms, mfu 46.73%
iter 3640: loss 0.1210, time 1609.95ms, mfu 46.89%
iter 3650: loss 0.1187, time 1574.71ms, mfu 47.13%
iter 3660: loss 0.1245, time 1699.05ms, mfu 46.99%
iter 3670: loss 0.1240, time 1569.37ms, mfu 47.25%
iter 3680: loss 0.1158, time 1568.87ms, mfu 47.48%
iter 3690: loss 0.1148, time 1632.22ms, mfu 47.49%
iter 3700: loss 0.1255, time 1636.16ms, mfu 47.49%
iter 3710: loss 0.1304, time 1603.21ms, mfu 47.59%
iter 3720: loss 0.1056, time 1601.46ms, mfu 47.68%
iter 3730: loss 0.1103, time 1572.86ms, mfu 47.86%
iter 3740: loss 0.1128, time 1683.34ms, mfu 47.69%
step 3750: train loss 0.0875, val loss 3.6168
iter 3750: loss 0.1109, time 60935.88ms, mfu 43.05%
iter 3760: loss 0.1161, time 1605.92ms, mfu 43.58%
iter 3770: loss 0.1169, time 1630.03ms, mfu 43.99%
iter 3780: loss 0.1231, time 1606.20ms, mfu 44.43%
iter 3790: loss 0.1301, time 1624.86ms, mfu 44.77%
iter 3800: loss 0.1030, time 1606.12ms, mfu 45.13%
iter 3810: loss 0.1093, time 1606.14ms, mfu 45.46%
iter 3820: loss 0.1170, time 1618.75ms, mfu 45.71%
iter 3830: loss 0.1176, time 1597.63ms, mfu 46.01%
iter 3840: loss 0.1176, time 1683.58ms, mfu 46.02%
iter 3850: loss 0.1161, time 1584.53ms, mfu 46.33%
iter 3860: loss 0.1141, time 1572.87ms, mfu 46.63%
iter 3870: loss 0.1155, time 1682.68ms, mfu 46.59%
iter 3880: loss 0.1146, time 1587.42ms, mfu 46.83%
iter 3890: loss 0.1126, time 1674.10ms, mfu 46.79%
iter 3900: loss 0.1099, time 1688.90ms, mfu 46.71%
iter 3910: loss 0.1073, time 1589.14ms, mfu 46.93%
iter 3920: loss 0.1080, time 1668.39ms, mfu 46.90%
iter 3930: loss 0.1062, time 1569.68ms, mfu 47.16%
iter 3940: loss 0.1051, time 1649.06ms, mfu 47.15%
iter 3950: loss 0.1054, time 1569.44ms, mfu 47.39%
iter 3960: loss 0.1154, time 1593.53ms, mfu 47.53%
iter 3970: loss 0.1252, time 1654.49ms, mfu 47.47%
iter 3980: loss 0.1079, time 1669.25ms, mfu 47.38%
iter 3990: loss 0.0995, time 1626.74ms, mfu 47.42%
step 4000: train loss 0.0834, val loss 3.7876
iter 4000: loss 0.1013, time 60967.81ms, mfu 42.81%
iter 4010: loss 0.0952, time 1606.12ms, mfu 43.37%
iter 4020: loss 0.1049, time 1606.30ms, mfu 43.87%
iter 4030: loss 0.1063, time 1606.07ms, mfu 44.32%
iter 4040: loss 0.1205, time 1655.75ms, mfu 44.58%
iter 4050: loss 0.1208, time 1633.54ms, mfu 44.88%
iter 4060: loss 0.1048, time 1606.02ms, mfu 45.23%
iter 4070: loss 0.1089, time 1664.05ms, mfu 45.38%
iter 4080: loss 0.1084, time 1661.91ms, mfu 45.52%
iter 4090: loss 0.1071, time 1592.90ms, mfu 45.85%
iter 4100: loss 0.1070, time 1566.40ms, mfu 46.22%
iter 4110: loss 0.1008, time 1664.13ms, mfu 46.27%
iter 4120: loss 0.1035, time 1686.74ms, mfu 46.25%
iter 4130: loss 0.1083, time 1552.09ms, mfu 46.63%
iter 4140: loss 0.1010, time 1716.59ms, mfu 46.50%
iter 4150: loss 0.1131, time 1591.26ms, mfu 46.73%
iter 4160: loss 0.1029, time 1689.01ms, mfu 46.66%
iter 4170: loss 0.1112, time 1572.86ms, mfu 46.94%
iter 4180: loss 0.1033, time 1686.52ms, mfu 46.85%
iter 4190: loss 0.1081, time 1695.09ms, mfu 46.75%
iter 4200: loss 0.1000, time 1574.43ms, mfu 47.01%
iter 4210: loss 0.1058, time 1684.09ms, mfu 46.93%
iter 4220: loss 0.1006, time 1535.97ms, mfu 47.29%
iter 4230: loss 0.1012, time 1524.67ms, mfu 47.66%
iter 4240: loss 0.0976, time 1605.86ms, mfu 47.74%
step 4250: train loss 0.0806, val loss 3.8271
iter 4250: loss 0.0950, time 61074.54ms, mfu 43.09%
iter 4260: loss 0.1036, time 1632.99ms, mfu 43.54%
iter 4270: loss 0.1036, time 1628.92ms, mfu 43.96%
iter 4280: loss 0.1074, time 1629.81ms, mfu 44.33%
iter 4290: loss 0.1014, time 1591.70ms, mfu 44.78%
iter 4300: loss 0.1061, time 1589.57ms, mfu 45.19%
iter 4310: loss 0.1087, time 1707.44ms, mfu 45.22%
iter 4320: loss 0.1014, time 1577.71ms, mfu 45.63%
iter 4330: loss 0.1063, time 1571.62ms, mfu 46.01%
iter 4340: loss 0.0899, time 1649.23ms, mfu 46.12%
iter 4350: loss 0.1072, time 1578.87ms, mfu 46.43%
iter 4360: loss 0.0978, time 1576.88ms, mfu 46.72%
iter 4370: loss 0.1122, time 1673.40ms, mfu 46.69%
iter 4380: loss 0.0926, time 1542.55ms, mfu 47.06%
iter 4390: loss 0.1003, time 1701.20ms, mfu 46.92%
iter 4400: loss 0.1018, time 1584.66ms, mfu 47.13%
iter 4410: loss 0.1047, time 1636.16ms, mfu 47.17%
iter 4420: loss 0.0935, time 1686.52ms, mfu 47.06%
iter 4430: loss 0.0940, time 1588.50ms, mfu 47.25%
iter 4440: loss 0.1083, time 1591.22ms, mfu 47.41%
iter 4450: loss 0.1011, time 1590.91ms, mfu 47.55%
iter 4460: loss 0.0875, time 1714.52ms, mfu 47.33%
iter 4470: loss 0.0938, time 1627.98ms, mfu 47.37%
iter 4480: loss 0.1006, time 1640.87ms, mfu 47.37%
iter 4490: loss 0.0963, time 1587.61ms, mfu 47.53%
step 4500: train loss 0.0776, val loss 3.9581
iter 4500: loss 0.1022, time 60956.82ms, mfu 42.90%
iter 4510: loss 0.1104, time 1606.21ms, mfu 43.45%
iter 4520: loss 0.0973, time 1634.14ms, mfu 43.86%
iter 4530: loss 0.0936, time 1606.10ms, mfu 44.32%
iter 4540: loss 0.0926, time 1624.21ms, mfu 44.67%
iter 4550: loss 0.1022, time 1628.69ms, mfu 44.97%
iter 4560: loss 0.0914, time 1572.74ms, mfu 45.42%
iter 4570: loss 0.0935, time 1679.05ms, mfu 45.51%
iter 4580: loss 0.0994, time 1614.06ms, mfu 45.77%
iter 4590: loss 0.0889, time 1661.99ms, mfu 45.87%
iter 4600: loss 0.0885, time 1589.69ms, mfu 46.17%
iter 4610: loss 0.1035, time 1591.68ms, mfu 46.44%
iter 4620: loss 0.0907, time 1669.12ms, mfu 46.45%
iter 4630: loss 0.0918, time 1577.78ms, mfu 46.73%
iter 4640: loss 0.1008, time 1568.51ms, mfu 47.01%
iter 4650: loss 0.0946, time 1687.06ms, mfu 46.92%
iter 4660: loss 0.1022, time 1574.92ms, mfu 47.16%
iter 4670: loss 0.0910, time 1605.72ms, mfu 47.29%
iter 4680: loss 0.0948, time 1626.96ms, mfu 47.33%
iter 4690: loss 0.0872, time 1605.98ms, mfu 47.44%
iter 4700: loss 0.0923, time 1605.88ms, mfu 47.54%
iter 4710: loss 0.0912, time 1625.31ms, mfu 47.56%
iter 4720: loss 0.0845, time 1605.66ms, mfu 47.65%
iter 4730: loss 0.0978, time 1626.51ms, mfu 47.66%
iter 4740: loss 0.0970, time 1606.41ms, mfu 47.73%
step 4750: train loss 0.0757, val loss 3.9880
iter 4750: loss 0.0975, time 60887.95ms, mfu 43.09%
iter 4760: loss 0.0946, time 1564.18ms, mfu 43.75%
iter 4770: loss 0.1051, time 1568.60ms, mfu 44.33%
iter 4780: loss 0.0995, time 1686.80ms, mfu 44.50%
iter 4790: loss 0.0927, time 1562.45ms, mfu 45.03%
iter 4800: loss 0.0996, time 1572.69ms, mfu 45.47%
iter 4810: loss 0.0912, time 1663.44ms, mfu 45.59%
iter 4820: loss 0.0913, time 1610.30ms, mfu 45.86%
iter 4830: loss 0.0871, time 1580.79ms, mfu 46.19%
iter 4840: loss 0.0846, time 1570.63ms, mfu 46.52%
iter 4850: loss 0.1026, time 1634.86ms, mfu 46.62%
iter 4860: loss 0.0821, time 1572.82ms, mfu 46.90%
iter 4870: loss 0.0979, time 1587.83ms, mfu 47.11%
iter 4880: loss 0.0947, time 1590.13ms, mfu 47.28%
iter 4890: loss 0.0959, time 1698.52ms, mfu 47.13%
iter 4900: loss 0.1019, time 1668.20ms, mfu 47.08%
iter 4910: loss 0.1012, time 1592.53ms, mfu 47.25%
iter 4920: loss 0.0851, time 1592.10ms, mfu 47.41%
iter 4930: loss 0.1029, time 1563.45ms, mfu 47.64%
iter 4940: loss 0.0917, time 1573.08ms, mfu 47.81%
iter 4950: loss 0.0755, time 1703.54ms, mfu 47.59%
iter 4960: loss 0.0909, time 1583.82ms, mfu 47.74%
iter 4970: loss 0.0891, time 1625.12ms, mfu 47.75%
iter 4980: loss 0.1006, time 1679.91ms, mfu 47.60%
iter 4990: loss 0.0894, time 1606.02ms, mfu 47.68%
step 5000: train loss 0.0743, val loss 4.0592
iter 5000: loss 0.0830, time 61028.03ms, mfu 43.04%
[RunLogger] Summary written to /pscratch/sd/e/es_lh/nanoGPT/results.csv
